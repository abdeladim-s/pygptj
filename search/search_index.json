{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PyGPT-J API Reference","text":""},{"location":"#pygptj.model","title":"pygptj.model","text":"<p>This module contains a simple Python API around gpt-j</p>"},{"location":"#pygptj.model.Model","title":"Model","text":"<pre><code>Model(\n    model_path,\n    prompt_context=\"\",\n    prompt_prefix=\"\",\n    prompt_suffix=\"\",\n    log_level=logging.ERROR,\n)\n</code></pre> <p>GPT-J model</p> <p>Example usage <pre><code>from pygptj.model import Model\n\nmodel = Model(ggml_model='path/to/ggml/model')\nfor token in model.generate(\"Tell me a joke ?\"):\n    print(token, end='', flush=True)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to a gpt-j <code>ggml</code> model</p> required <code>prompt_context</code> <code>str</code> <p>the global context of the interaction</p> <code>''</code> <code>prompt_prefix</code> <code>str</code> <p>the prompt prefix</p> <code>''</code> <code>prompt_suffix</code> <code>str</code> <p>the prompt suffix</p> <code>''</code> <code>log_level</code> <code>int</code> <p>logging level</p> <code>logging.ERROR</code> Source code in <code>pygptj/model.py</code> <pre><code>def __init__(self,\n             model_path: str,\n             prompt_context: str = '',\n             prompt_prefix: str = '',\n             prompt_suffix: str = '',\n             log_level: int = logging.ERROR):\n\"\"\"\n    :param model_path: The path to a gpt-j `ggml` model\n    :param prompt_context: the global context of the interaction\n    :param prompt_prefix: the prompt prefix\n    :param prompt_suffix: the prompt suffix\n    :param log_level: logging level\n    \"\"\"\n    # set logging level\n    set_log_level(log_level)\n    self._ctx = None\n\n    if not Path(model_path).is_file():\n        raise Exception(f\"File {model_path} not found!\")\n\n    self.model_path = model_path\n\n    self._model = pp.gptj_model()\n    self._vocab = pp.gpt_vocab()\n\n    # load model\n    self._load_model()\n\n    # gpt params\n    self.gpt_params = pp.gptj_gpt_params()\n    self.hparams = pp.gptj_hparams()\n\n    self.res = \"\"\n\n    self.logits = []\n\n    self._n_past = 0\n    self.prompt_cntext = prompt_context\n    self.prompt_prefix = prompt_prefix\n    self.prompt_suffix = prompt_suffix\n\n    self._prompt_context_tokens = []\n    self._prompt_prefix_tokens = []\n    self._prompt_suffix_tokens = []\n\n    self.reset()\n</code></pre>"},{"location":"#pygptj.model.Model.generate","title":"generate","text":"<pre><code>generate(\n    prompt,\n    n_predict=None,\n    antiprompt=None,\n    seed=None,\n    n_threads=4,\n    top_k=40,\n    top_p=0.9,\n    temp=0.9,\n)\n</code></pre> <p>Runs GPT-J inference and yields new predicted tokens</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt :)</p> required <code>n_predict</code> <code>Union[None, int]</code> <p>if n_predict is not None, the inference will stop if it reaches <code>n_predict</code> tokens, otherwise it will continue until <code>end of text</code> token</p> <code>None</code> <code>antiprompt</code> <code>str</code> <p>aka the stop word, the generation will stop if this word is predicted, keep it None to handle it in your own way</p> <code>None</code> <code>seed</code> <code>int</code> <p>random seed</p> <code>None</code> <code>n_threads</code> <code>int</code> <p>The number of CPU threads</p> <code>4</code> <code>top_k</code> <code>int</code> <p>top K sampling parameter</p> <code>40</code> <code>top_p</code> <code>float</code> <p>top P sampling parameter</p> <code>0.9</code> <code>temp</code> <code>float</code> <p>temperature</p> <code>0.9</code> <p>Returns:</p> Type Description <code>Generator</code> <p>Tokens generator</p> Source code in <code>pygptj/model.py</code> <pre><code>def generate(self,\n             prompt: str,\n             n_predict: Union[None, int] = None,\n             antiprompt: str = None,\n             seed: int = None,\n             n_threads: int = 4,\n             top_k: int = 40,\n             top_p: float = 0.9,\n             temp: float = 0.9,\n             ) -&gt; Generator:\n\"\"\"\n     Runs GPT-J inference and yields new predicted tokens\n\n    :param prompt: The prompt :)\n    :param n_predict: if n_predict is not None, the inference will stop if it reaches `n_predict` tokens, otherwise\n                      it will continue until `end of text` token\n    :param antiprompt: aka the stop word, the generation will stop if this word is predicted,\n                       keep it None to handle it in your own way\n    :param seed: random seed\n    :param n_threads: The number of CPU threads\n    :param top_k: top K sampling parameter\n    :param top_p: top P sampling parameter\n    :param temp: temperature\n\n    :return: Tokens generator\n    \"\"\"\n    if seed is None or seed &lt; 0:\n        seed = int(time.time())\n\n    logging.info(f'seed = {seed}')\n\n    if self._n_past == 0 or antiprompt is None:\n        # add the prefix to the context\n        embd_inp = self._prompt_prefix_tokens + pp.gpt_tokenize(self._vocab, prompt) + self._prompt_suffix_tokens\n    else:\n        # do not add the prefix again as it is already in the previous generated context\n        embd_inp = pp.gpt_tokenize(self._vocab, prompt) + self._prompt_suffix_tokens\n\n    if n_predict is not None:\n        n_predict = min(n_predict, self.hparams.n_ctx - len(embd_inp))\n    logging.info(f'Number of tokens in prompt = {len(embd_inp)}')\n\n    embd = []\n    # add global context for the first time\n    if self._n_past == 0:\n        for tok in self._prompt_context_tokens:\n            embd.append(tok)\n\n    # consume input tokens\n    for tok in embd_inp:\n        embd.append(tok)\n\n    # determine the required inference memory per token:\n    mem_per_token = 0\n    logits, mem_per_token = pp.gptj_eval(self._model, n_threads, 0, [0, 1, 2, 3], mem_per_token)\n\n    i = len(embd) - 1\n    id = 0\n    if antiprompt is not None:\n        sequence_queue = []\n        stop_word = antiprompt.strip()\n\n    while id != 50256:  # end of text token\n        if n_predict is not None:  # break the generation if n_predict\n            if i &gt;= (len(embd_inp) + n_predict):\n                break\n        i += 1\n        # predict\n        if len(embd) &gt; 0:\n            try:\n                logits, mem_per_token = pp.gptj_eval(self._model, n_threads, self._n_past, embd, mem_per_token)\n                self.logits.append(logits)\n            except Exception as e:\n                print(f\"Failed to predict\\n {e}\")\n                return\n\n        self._n_past += len(embd)\n        embd.clear()\n\n        if i &gt;= len(embd_inp):\n            # sample next token\n            n_vocab = self.hparams.n_vocab\n            t_start_sample_us = int(round(time.time() * 1000000))\n            id = pp.gpt_sample_top_k_top_p(self._vocab, logits[-n_vocab:], top_k, top_p, temp, seed)\n            if id == 50256:  # end of text token\n                break\n            # add the token to the context\n            embd.append(id)\n            token = self._vocab.id_to_token[id]\n            # antiprompt\n            if antiprompt is not None:\n                if token == '\\n':\n                    sequence_queue.append(token)\n                    continue\n                if len(sequence_queue) != 0:\n                    if stop_word.startswith(''.join(sequence_queue).strip()):\n                        sequence_queue.append(token)\n                        if ''.join(sequence_queue).strip() == stop_word:\n                            break\n                        else:\n                            continue\n                    else:\n                        # consume sequence queue tokens\n                        while len(sequence_queue) != 0:\n                            yield sequence_queue.pop(0)\n                        sequence_queue = []\n\n            yield token\n</code></pre>"},{"location":"#pygptj.model.Model.cpp_generate","title":"cpp_generate","text":"<pre><code>cpp_generate(\n    prompt,\n    new_text_callback=None,\n    logits_callback=None,\n    n_predict=128,\n    seed=-1,\n    n_threads=4,\n    top_k=40,\n    top_p=0.9,\n    temp=0.9,\n    n_batch=8,\n)\n</code></pre> <p>Runs the inference to cpp generate function</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>the prompt</p> required <code>new_text_callback</code> <code>Callable[[str], None]</code> <p>a callback function called when new text is generated, default <code>None</code></p> <code>None</code> <code>logits_callback</code> <code>Callable[[np.ndarray], None]</code> <p>a callback function to access the logits on every inference</p> <code>None</code> <code>n_predict</code> <code>int</code> <p>number of tokens to generate</p> <code>128</code> <code>seed</code> <code>int</code> <p>The random seed</p> <code>-1</code> <code>n_threads</code> <code>int</code> <p>Number of threads</p> <code>4</code> <code>top_k</code> <code>int</code> <p>top_k sampling parameter</p> <code>40</code> <code>top_p</code> <code>float</code> <p>top_p sampling parameter</p> <code>0.9</code> <code>temp</code> <code>float</code> <p>temperature sampling parameter</p> <code>0.9</code> <code>n_batch</code> <code>int</code> <p>batch size for prompt processing</p> <code>8</code> <p>Returns:</p> Type Description <code>str</code> <p>the new generated text</p> Source code in <code>pygptj/model.py</code> <pre><code>def cpp_generate(self,\n                 prompt: str,\n                 new_text_callback: Callable[[str], None] = None,\n                 logits_callback: Callable[[np.ndarray], None] = None,\n                 n_predict: int = 128,\n                 seed: int = -1,\n                 n_threads: int = 4,\n                 top_k: int = 40,\n                 top_p: float = 0.9,\n                 temp: float = 0.9,\n                 n_batch: int = 8,\n                 ) -&gt; str:\n\"\"\"\n    Runs the inference to cpp generate function\n\n    :param prompt: the prompt\n    :param new_text_callback: a callback function called when new text is generated, default `None`\n    :param logits_callback: a callback function to access the logits on every inference\n    :param n_predict: number of tokens to generate\n    :param seed: The random seed\n    :param n_threads: Number of threads\n    :param top_k: top_k sampling parameter\n    :param top_p: top_p sampling parameter\n    :param temp: temperature sampling parameter\n    :param n_batch: batch size for prompt processing\n\n    :return: the new generated text\n    \"\"\"\n    self.gpt_params.prompt = prompt\n    self.gpt_params.n_predict = n_predict\n    self.gpt_params.seed = seed\n    self.gpt_params.n_threads = n_threads\n    self.gpt_params.top_k = top_k\n    self.gpt_params.top_p = top_p\n    self.gpt_params.temp = temp\n    self.gpt_params.n_batch = n_batch\n\n    # assign new_text_callback\n    self.res = \"\"\n    Model._new_text_callback = new_text_callback\n\n    # assign _logits_callback used for saving logits, token by token\n    Model._logits_callback = logits_callback\n\n    # run the prediction\n    pp.gptj_generate(self.gpt_params, self._model, self._vocab, self._call_new_text_callback,\n                     self._call_logits_callback)\n    return self.res\n</code></pre>"},{"location":"#pygptj.model.Model.braindump","title":"braindump","text":"<pre><code>braindump(path)\n</code></pre> <p>Dumps the logits to .npy</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output path</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>pygptj/model.py</code> <pre><code>def braindump(self, path: str) -&gt; None:\n\"\"\"\n    Dumps the logits to .npy\n    :param path: Output path\n    :return: None\n    \"\"\"\n    np.save(path, np.asarray(self.logits))\n</code></pre>"},{"location":"#pygptj.model.Model.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Resets the context</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>pygptj/model.py</code> <pre><code>def reset(self) -&gt; None:\n\"\"\"\n    Resets the context\n    :return: None\n    \"\"\"\n    self._n_past = 0\n    self._prompt_context_tokens = pp.gpt_tokenize(self._vocab, self.prompt_cntext)\n    self._prompt_prefix_tokens = pp.gpt_tokenize(self._vocab, self.prompt_prefix)\n    self._prompt_suffix_tokens = pp.gpt_tokenize(self._vocab, self.prompt_suffix)\n</code></pre>"},{"location":"#pygptj.model.Model.get_params","title":"get_params  <code>staticmethod</code>","text":"<pre><code>get_params(params)\n</code></pre> <p>Returns a <code>dict</code> representation of the params</p> <p>Returns:</p> Type Description <code>dict</code> <p>params dict</p> Source code in <code>pygptj/model.py</code> <pre><code>@staticmethod\ndef get_params(params) -&gt; dict:\n\"\"\"\n    Returns a `dict` representation of the params\n    :return: params dict\n    \"\"\"\n    res = {}\n    for param in dir(params):\n        if param.startswith('__'):\n            continue\n        res[param] = getattr(params, param)\n    return res\n</code></pre>"}]}